<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://chenat9.github.io</id>
    <title>Fly</title>
    <updated>2022-08-17T06:28:23.749Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://chenat9.github.io"/>
    <link rel="self" href="https://chenat9.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://chenat9.github.io/images/avatar.png</logo>
    <icon>https://chenat9.github.io/favicon.ico</icon>
    <rights>All rights reserved 2022, Fly</rights>
    <entry>
        <title type="html"><![CDATA[第一封]]></title>
        <id>https://chenat9.github.io/post/di-yi-feng/</id>
        <link href="https://chenat9.github.io/post/di-yi-feng/">
        </link>
        <updated>2022-08-17T06:26:12.000Z</updated>
        <content type="html"><![CDATA[<h1 id="我一直在想自己究竟为什么喜欢你">我一直在想，自己究竟为什么喜欢你。</h1>
<p>也许是那天在清冷的公园里，第一次牵你手的时候，心跳得热烈。你总是问我是不是谈了太多恋爱就不会再心动了，我不知道以后我会给出什么样的答案，但是和你在一起的时候，心动过好多次。</p>
<p>喜欢你是一件很开心的事，想见到你时的冲动，可以连工作都不做了；很冷的天还是会想和你在那条马路上散步；把上班时间提前一个小时就为了和你在地铁上牵手几分钟。很多时候，我都在很认真很认真的喜欢着你，像一个第一次谈恋爱的小男生，不懂节制，热烈奔放的喜欢。</p>
<h1 id="情不知所起一往而深">情不知所起，一往而深。</h1>
<p>喜欢的太深就会有负作用，争吵和矛盾其实大多数都是源自于我，对感情的纯粹度要求太高，没有得到及时反馈就情绪波动。感情里的我一向不够理智，也曾去寻找各种情感理论来指导我应该怎样喜欢你。他们告诉我，依赖型人格会有这样那样的毛病，缺爱，患得患失，情绪失控，需要被满足。我也想改掉这一切的问题和缺陷，心理学说要降低对你的依附，情感大师说要不以物喜不以己悲，佛祖说放下贪痴嗔念欲。我说，去你们妈的，我就是放不下一点点对付鑫怡的喜欢。</p>
<p>你总是说我对你提了太多要求，其实很多时候，我只是想我和你的恋爱能够留下很多只有我跟你独有的记忆。单纯的想法，却没有很好的表达给你，我应该跟你道歉，但我还是想做只有我和你做过的事。真真切切的爱过一遍，就应该留下一点记忆和联系。不然如果因为意外结束这段感情，连哭着删朋友圈的机会都没有，那我多可悲，没有存在过的证据。</p>
<h1 id="长恨人心不如水等闲平地起波澜">长恨人心不如水，等闲平地起波澜。</h1>
<p>尽管岁数不小，但是恋爱心智还不够成熟。我认为这不是坏事，也不想听到你指责我幼稚，毕竟就像是第一次喜欢一个人，把自己的心全部交给你的那种笨拙和真诚，与我而言，这很值得，并且很酷。</p>
<p>我们应该就会在这座2000万人口的城市里一直生活下去，在那个房子里，种上你喜欢的花，再养一只猫。每一个清晨，我会拉开碎花窗帘轻轻叫醒你，一起刷牙吃早餐然后拥抱去上班。这样平淡的日子我设想过无数次，最重要的是，你也在。跟你恋爱，然后成长，被你信任理解和依赖，这应该也是人世间最浪漫的事情之一，也是我们能够共度余生的基石。</p>
<p>多给我一点回应，接下来在恋爱的这条路上，就让我牵着你的手走到最后吧。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ristretto -- 高性能Go Cache]]></title>
        <id>https://chenat9.github.io/post/ristretto-gao-xing-neng-go-cache/</id>
        <link href="https://chenat9.github.io/post/ristretto-gao-xing-neng-go-cache/">
        </link>
        <updated>2021-12-19T16:11:43.000Z</updated>
        <content type="html"><![CDATA[<h3 id="前言">前言</h3>
<p>故事开始于我们的Github项目<a href="https://github.com/dgraph-io/dgraph">Dgraph 中</a>需要一个内存受限、支持并发的 Go 缓存库。我们四处寻找但没有找到一个很好的解决方案。随后我们尝试使用基于分片的HashMap，通过每个分片独立的淘汰策略来释放内存，这引发了一些内存问题。然后我们启用了 Groupcache 的<a href="https://github.com/golang/groupcache/blob/master/lru/lru.go">LRU</a>，它使用互斥锁来保证线程安全。使用一年后，我们注意到这个缓存库的锁冲突情况比较严重。有一次，我们提了Commit从代码中去掉这个缓存，结果令查询性能提高了5-10倍。也就是说**，我们的缓存反而拖累了我们！**</p>
<p>我们得出结论，在Go的生态中，支持并发操作的缓存不完整的，需要被尽快补充完整。三月，我们写了关于<a href="https://blog.dgraph.io/post/caching-in-go/">Go 缓存状态的文章</a>，提到了数据库和系统需要一个智能的、内存受限的缓存，该缓存可以扩展到 Go 程序所处的多线程环境。我们给这个缓存库提出了如下的要求：</p>
<ol>
<li>支持并发操作</li>
<li>高缓存命中率</li>
<li>内存受限（限制为配置文件中指定的最大内存使用量）</li>
<li>随着CPU核心数和Goroutine数的增加，具有很好的扩展性</li>
<li>在非随机Key访问分布下（例如 Zipf）下，具有很好的扩展性</li>
</ol>
<p>发布<a href="https://blog.dgraph.io/post/caching-in-go/">博文后</a>，我们组建了一个团队来解决其中提到的挑战，并创建了一个值得与非 Go 缓存实现进行比较的 Go 缓存。值得一提的是<a href="https://github.com/ben-manes/caffeine">Caffeine</a>，它是一个基于 Java 8 的高性能、近乎最优的缓存库。许多基于 Java 的数据库都在使用它，例如 Cassandra、HBase 和 Neo4j。关于Caffeine的设计文章可以查看这里。</p>
<h3 id="ristretto更好的浓缩咖啡">Ristretto：更好的浓缩咖啡</h3>
<p>从那以后，我们阅读文献、广泛测试了各种Cache实现，并讨论了在编写缓存库时需要考虑的方方面面。今天，我们自豪地宣布，它已经准备好被Go社区更广泛的使用和测试。</p>
<p>在我们开始解释<a href="https://github.com/dgraph-io/ristretto">Ristretto</a>的设计之前，这里有一个代码片段，展示了它的用法：</p>
<pre><code class="language-go">func main() {
	cache, err := ristretto.NewCache(&amp;ristretto.Config{
		NumCounters: 1e7,     // Num keys to track frequency of (10M).
		MaxCost:     1 &lt;&lt; 30, // Maximum cost of cache (1GB).
		BufferItems: 64,      // Number of keys per Get buffer.
	})
	if err != nil {
		panic(err)
	}

	cache.Set(&quot;key&quot;, &quot;value&quot;, 1) // set a value
	// wait for value to pass through buffers
	time.Sleep(10 * time.Millisecond)

	value, found := cache.Get(&quot;key&quot;)
	if !found {
		panic(&quot;missing value&quot;)
	}
	fmt.Println(value)
	cache.Del(&quot;key&quot;)
}
</code></pre>
<h4 id="指导原则">指导原则</h4>
<p><a href="https://github.com/dgraph-io/ristretto">Ristretto</a>建立在三个指导原则之上：</p>
<ol>
<li>快速访问</li>
<li>高并发、抗锁冲突</li>
<li>内存受限</li>
</ol>
<p>在这篇博文中，我们将讨论这三个原则的具体含义，以及我们如何在 Ristretto 中达到这些原则。</p>
<h3 id="快速访问">快速访问</h3>
<p>尽管我们喜欢 Go 及其对功能的固执己见，但一些 Go 设计决策导致我们无法榨干我们想要实现的所有性能。尤其是是 Go 的并发模型。由于对CSP的关注，大多数其他形式的原子操作都被忽略了。这使得实现一些对缓存很有用的无锁结构是一件很困难的事情。例如，Go <a href="https://groups.google.com/d/msg/golang-nuts/M9kF6Tdh2Vo/3tLSFYYOGgAJ">不</a>提供线程本地存储（Thread-Local）。</p>
<p>从本质上讲，Cache就是简单的HashMap，同时包含一些关于数据准入和淘汰的规则。如果HashMap性能表现不佳，那么整个Cache都会受到影响。与 Java 不同，Go 没有ConcurrentHashMap这种东西（大意是分段加锁，降低锁冲突）。相反，Go 中的线程安全是通过显式的获取互斥锁来实现的。</p>
<p>我们尝试了多种实现，发现<code>sync.Map</code>在频繁的Read操作负载下表现良好，但在Write操作负载时表现不佳。考虑到没有Thread-Local 存储， <strong>我们发现分片且带有互斥锁的 Go Map 具有最佳的整体性能。</strong> 我们选择使用 256 个分片以确保即使在 64 核服务器上也可以获得很好的表现。</p>
<p>要想使用这种基于分片的方式，我们还需要找到一种方法快速计算Hash应该进入哪个分片。这样的需求，以及担心Long Key（比较大的Key值）会消耗过多内存，我们决定对Key使用<code>uint64</code>代替，而不是存储整个Key。基本原理是在入口处对Key执行一次Hash计算，这个Hash值允许在后面的多处使用，避免更多计算。</p>
<p><strong>为了快速生成Hash，我们从 Go Runtime借用了<a href="https://github.com/dgraph-io/ristretto/blob/master/z/rtutil.go#L42-L44">runtime.memhash</a>。</strong> 该函数使用汇编代码快速生成哈希。它在进程启动时会初始化一个随机数发生器，这意味着相同的Key可能在下一次进程启动时生成不同的Hash值。不过这对于非持久性缓存来说是没问题的。在我们的<a href="https://github.com/dgraph-io/ristretto/blob/master/z/rtutil_test.go#L11-L44">实验中</a>，我们发现它可以在 10ns 内散列 64 字节的密钥。</p>
<pre><code>BenchmarkMemHash-32 200000000	 8.88 ns/op
BenchmarkFarm-32    100000000	 17.9 ns/op
BenchmarkSip-32      30000000	 41.1 ns/op
BenchmarkFnv-32      20000000	 70.6 ns/op
</code></pre>
<p>我们不仅将这个Hash用作存储的Key，而且还用于确定Key应该进入的分片。<em>这带来了Hash冲突的可能，在接下来的文章中我们会讲解如何解决它。</em></p>
<h3 id="支持并发和抗争用">支持并发和抗争用</h3>
<p>实现高命中率需要管理一些元数据，这些元数据用来表示Cache中当前存在哪些数据，以及Cache中应该存在哪些数据。支持多Goroutine的Cache中，平衡缓存的性能和可扩展性变得非常困难。幸运的是，有<a href="https://dgraph.io/blog/refs/bp_wrapper.pdf">一篇</a>名为<em>BP-Wrapper</em>的论文写了一个系统框架，使任何替换算法几乎无锁争用。该论文描述了两种缓解争用的方法：<em>预取</em>和 <em>批处理</em> 。我们只使用批处理。</p>
<p>批处理的工作方式和你想象的非常相似。 它<strong>不是为每次的元数据变化获取互斥锁，而是先等待RingBuffer填满，再去获取互斥锁处理元数据的变化。</strong> 正如论文中所述，这大大降低了锁冲突，而且开销很小。</p>
<p>我们在所有的Get操作和更新到缓存中的Set操作上使用了这种思想。</p>
<h4 id="get请求">Get请求</h4>
<p>当然，所有Get请求都会立即得到响应（不需要等待缓冲区填满）。我们只是用来统计Get操作的次数，因此必须追踪对Key的访问。在 LRU 缓存中，通常将Key放置在链表的头部。在基于 LFU 的缓存中，我们需要增加Key的命中计数器。这两个操作都需要对Cache的全局结构进行访问，而且必须是线程安全的。<a href="https://dgraph.io/blog/refs/bp_wrapper.pdf">BP-Wrapper</a>建议使用批处理来处理Key的命中计数器，但问题是我们如何在不获取另一个锁的情况下实现这个批处理过程。</p>
<p>这听起来像是 Go Channel的完美应用，确实如此。不幸的是，通道的吞吐量性能太低阻碍了我们的使用。 <strong>我们设计使用<code>sync.Pool</code>实现来条带化的，但有损<a href="https://github.com/dgraph-io/ristretto/blob/master/ring.go#L99-L104">环形缓冲区</a></strong> ，该缓冲区具有出色的性能且数据丢失很少。</p>
<p>存储在Pool中的任何数据可能会随时自动删除，而没有任何通知。这个操作导致了一级有损行为*。* Pool 中的每一项实际上都是一批Key。当批次填满时，它会被推送到一个Channel。Channel大小故意保持较小，以避免消耗太多 CPU 周期来处理它。如果通道已满，则丢弃该批次。  <em>这引入了二级有损行为。</em> 一个 Goroutine 从Channel中获取并处理这批Key，更新它们的命中计数器。</p>
<pre><code class="language-go">AddToLossyBuffer(key):
  stripe := b.pool.Get().(*ringStripe)
  stripe.Push(key)
  b.pool.Put(stripe)

Once buffer fills up, push to channel:
  select {
  case p.itemsCh &lt;- keys:
      p.stats.Add(keepGets, keys[0], uint64(len(keys)))
      return true
  default:
      p.stats.Add(dropGets, keys[0], uint64(len(keys)))
      return false
  }

p.itemCh processing:
  func (p *tinyLFU) Push(keys []uint64) {
    for _, key := range keys {
      p.Increment(key)
    }
  }
</code></pre>
<p>使用<code>sync.Pool</code>优于其他任何东西（slice、striped mutexes等）, 由于是Thread-Local存储的内部使用，所以会有这么大的性能优势。<em>这些东西不能作为公共 API 提供给 Go 用户。</em></p>
<h4 id="set操作">Set操作</h4>
<p>Set 缓冲区的要求与 Get 略有不同。在 Gets 中，我们只缓冲Key，只有在缓冲区填满后才处理它们。在 Sets 中，我们希望尽快处理这些Key。 <strong>因此，我们使用一个通道来捕获 Sets，如果通道已满，则丢弃它们上以避免争用。</strong> 几个后台 Goroutines 从Channel中取回并且处理Set操作。</p>
<p>这种方法与 Gets 一样，旨在优化抗争用。但是，有一些注意事项，如下所述。</p>
<pre><code class="language-go">select {
case c.setBuf &lt;- &amp;item{key: hash, val: val, cost: cost}:
    return true
default:
    // drop the set and avoid blocking
    c.stats.Add(dropSets, hash, 1)
    return false
}
</code></pre>
<h4 id="注意事项">注意事项</h4>
<p><a href="https://github.com/dgraph-io/ristretto">Ristretto</a>中的Set操作排队进入Buffer，控制权返回给调用者，然后Buffer被应用到缓存中。这有两个副作用：</p>
<ol>
<li>一次Set<strong>操作不保证一定会为apply。</strong> 可能立即放弃以避免争用，也可以稍后被策略拒绝。</li>
<li>即使Apply了 Set操作，调用返回给用户后也可能需要几毫秒的时间。在数据库术语中，它是一种<em>最终一致性</em> 模型。</li>
</ol>
<p>但是，如果缓存中已经存在该Key，则 Set 将立即更新Key。这是为了避免缓存的Key保存过时的值。</p>
<h4 id="降低锁冲突">降低锁冲突</h4>
<p><strong>Ristretto 针对降低锁冲突进行了优化。</strong> 这在高并发负载下表现非常好，我们将在下面的吞吐量Benchmark中看到。但是，它会丢失一些元数据以换取更好的吞吐量性能。</p>
<p>有趣的是，由于Hash访问分布的性质，信息丢失并不会影响我们的命中率性能。如果我们确实丢失了元数据，它通常会均匀丢失，而密钥访问分布仍然不均匀。因此，我们仍然实现了高命中率，并且命中率下降很小，如下图所示。</p>
<figure data-type="image" tabindex="1"><img src="https://dgraph.io/blog/images/rt-hit-degrade.svg?sanitize=true" alt="Ristretto 命中率下降" loading="lazy"></figure>
<h3 id="内存受限">内存受限</h3>
<h4 id="key的成本">Key的成本</h4>
<p>不可能存在无限大的缓存*。* 缓存必须有大小限制。许多缓存库将缓存大小视为元素的数量。我们发现这种方法是不成熟的 。当然，它适用于值大小相同的负载。但是，大多数情况下负载值的大小并不确定。一个值占用的空间可能是Bytes、KB或者MB。将他们视为消耗相同的内存成本是不合理的。</p>
<p><strong>在<a href="https://github.com/dgraph-io/ristretto">Ristretto 中</a>，我们为每个键值附加一个成本的概念。</strong> 用户可以在调用 Set 时指定成本是多少。我们根据缓存的MaxCost配置项计算此成本。当Cache满负荷运行时，一个占用内存大的数据可能会取代许多占用内存少的数据。这是一种很好的机制，因为它适用于不同的工作负载，包括每个Key-Value成本为 1 的简单方法。</p>
<h4 id="tinylfu-准入策略">TinyLFU 准入策略</h4>
<blockquote>
<p>“应该让什么样的数据进入Cache？”</p>
</blockquote>
<p>这个答案应该有准入策略给出。显而易见的是，我们想要实现的目标是，新数据的进入比旧数据更 <em>“有价值”</em> 。然而，如何跟踪与 <em>“价值”</em> 问题相关的这些数据信息所需的开销（延迟和内存），这将成为一个挑战。</p>
<p>虽然准入策略可以提高Cache命中率，但大多数 Go 缓存库根本没有提供准入策略。许多 LRU 淘汰策略的实现假设最新的Key是最有价值的。</p>
<p>此外，大多数 Go 缓存库使用纯 LRU 或 LRU 的变种作为其淘汰策略。尽管 LRU算法的情况不错，但某些工作负载更适合 LFU 淘汰策略。我们使用各种各样的追踪手段在基准测试中证实了这一点。</p>
<p>对于我们的准入策略，我们阅读了**<a href="https://dgraph.io/blog/refs/TinyLFU%20-%20A%20Highly%20Efficient%20Cache%20Admission%20Policy.pdf">TinyLFU 的</a>**这篇引人入胜的新论文 ：一种高效的缓存准入策略。TinyLFU 提供了三个方法：</p>
<ul>
<li>Incremenmt</li>
<li>Estimate(key uint64) int（简称ɛ）</li>
<li>Reset</li>
</ul>
<p>该<a href="https://dgraph.io/blog/refs/TinyLFU%20-%20A%20Highly%20Efficient%20Cache%20Admission%20Policy.pdf">论文</a>做了很好的解释，但 TinyLFU 是一种与淘汰策略无关的策略，旨在以很少的内存开销提高命中率。主要思想是 <strong>仅在新数据的估计值高于被淘汰数据的估计值时才让其进入。</strong> 我们 使用Count-Min Sketch在<a href="https://github.com/dgraph-io/ristretto">Ristretto 中</a>实现了 TinyLFU 。它使用 4 位计数器来估算项目的访问频率 (ɛ)。每个键的这种小成本使我们能够跟踪全局键空间的更大样本，这比使用普通键到频率映射可能实现的要大得多。<a href="https://en.wikipedia.org/wiki/Count%E2%80%93min_sketch"></a></p>
<p>TinyLFU 还通过<code>Reset</code>函数维护Key访问的新近度。在 N 个Key递增后，计数器减半。因此，一段时间未看到的Key会将其计数器重置为零；为最近看到的Key铺平了道路。</p>
<h4 id="simpled-lfu-淘汰策略">Simpled LFU 淘汰策略</h4>
<p>当缓存达到容量时，每个传入的Key都可能替换缓存中存在的一个或多个键。不仅如此， <strong>传入key的ɛ应该高于被驱逐的key的ɛ。</strong> 为了找到具有低 ɛ 的键，我们使用Go Map迭代器提供的自然 <a href="https://blog.golang.org/go-maps-in-action">随机性</a>来选择Key样本并循环它们以找到具有最低 ɛ 的键。</p>
<p>然后我们将这个Kye的 ɛ 与传入的Key进行比较。如果传入的密钥具有更高的 ɛ，则该Key将被淘汰（ 淘汰<em>策略</em> ）。否则，传入的Key将被拒绝（ <em>准入</em>策略 ）。重复此机制，直到传入Key的成本可以放入缓存中。因此，单个传入Key可能会取代多个Key。<em>请注意，传入Key的成本不会影响选择淘汰Key的因素。</em></p>
<p><strong>使用这种方法，对于各种工作负载，命中率在准确 LFU 策略的 1% 以内。</strong> 这意味着我们在同一个小包中获得了准入策略、保守的内存使用和更低的争用的好处。</p>
<pre><code class="language-go">// Snippet from the Admission and Eviction Algorithm
incHits := p.admit.Estimate(key)
for ; room &lt; 0; room = p.evict.roomLeft(cost) {
    sample = p.evict.fillSample(sample)
    minKey, minHits, minId := uint64(0), int64(math.MaxInt64), 0
    for i, pair := range sample {
        if hits := p.admit.Estimate(pair.key); hits &lt; minHits {
            minKey, minHits, minId = pair.key, hits, i
        }
    }
    if incHits &lt; minHits {
        p.stats.Add(rejectSets, key, 1)
        return victims, false
    }
    p.evict.del(minKey)
    sample[minId] = sample[len(sample)-1]
    sample = sample[:len(sample)-1]
    victims = append(victims, minKey)
}
</code></pre>
<h4 id="守门员">守门员</h4>
<p>在我们在 TinyLFU 中放置新Key之前，<a href="https://github.com/dgraph-io/ristretto">Ristretto</a>使用布隆过滤器首先检查该密钥之前是否被看到过。只有当密钥已经存在于布隆过滤器中时，它才会被插入到 TinyLFU 中。这是为了避免使用不 多次出现的长尾键<em>污染</em>TinyLFU。</p>
<p>在计算一个键的ɛ时，如果该项目包含在布隆过滤器中，则其频率估计为来自 TinyLFU 的估计值加一。在<code>Reset</code>TinyLFU期间 ，布隆过滤器也被清除。</p>
<h3 id="指标">指标</h3>
<p>这是一个可选的配置项，但对于了解缓存的行为方式很重要。我们希望确保与缓存相关的跟踪指标不仅是可能的，而且这样做的开销足够低，可以打开和保持。</p>
<p>除了命中和未命中之外，Ristretto 还跟踪诸如Key及其Cost被添加、更新和淘汰、Set操作被丢弃或拒绝以及被丢弃或保留等指标。所有这些数字都有助于了解各种工作负载上的缓存行为，并为进一步优化铺平道路。</p>
<p>我们最初为这些使用原子计数器。但是，开销很大。我们将原因定位到<a href="https://dzone.com/articles/false-sharing">False Sharing</a>。在一个多核系统只不过，其中不同的原子计数器（每个 8 字节）位于同一Cache line（通常为 64 字节）中。对这些计数器之一进行的任何更新都会导致其他计数器被标记为 <em>invalid</em> 。这会强制为所有其他持有该缓存的内核重新加载缓存，从而在缓存行上产生写入争用。</p>
<p><strong>为了实现可扩展性，我们确保每个原子计数器完全占用一个完整的Cache line。</strong> 因此，每个内核都在不同的缓存行上工作。Ristretto 通过为每个Metrics分配 256 个 uint64 来使用它，在每个活动 uint64 之间留下 9 个未使用的 uint64。为了避免额外的计算，Key散列被重用以确定要增加哪个 uint64。</p>
<pre><code class="language-go">Add:
	valp := p.all[t]
	// Avoid false sharing by padding at least 64 bytes of space between two
	// atomic counters which would be incremented.
	idx := (hash % 25) * 10
	atomic.AddUint64(valp[idx], delta)

Read:
	valp := p.all[t]
	var total uint64
	for i := range valp {
		total += atomic.LoadUint64(valp[i])
	}
	return total
</code></pre>
<p>读取metric时，读取所有的uint64s并求和得到最新的数字。<strong>使用这种方法，指标跟踪只会为缓存性能增加大约 10% 的开销。</strong></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Golang Pipeline]]></title>
        <id>https://chenat9.github.io/post/golang-pipeline/</id>
        <link href="https://chenat9.github.io/post/golang-pipeline/">
        </link>
        <updated>2021-02-05T08:16:23.000Z</updated>
        <content type="html"><![CDATA[<!-- more -->
<p>本文介绍 Golang 中的一种并发编程模式，Pipeline 流水线模式。</p>
<h4 id="什么是流水线模式">什么是流水线模式？</h4>
<p>流水线顾名思义，即一系列处理单元组成的一套处理逻辑。</p>
<h4 id="为什么需要流水线模式">为什么需要流水线模式？</h4>
<p>编程当然可以写成串行的方式，但是这样带来的缺点是，如果某一个阶段耗时比较久，将会导致CPU空闲，这一点就不必多说了，因此引入了并发编程。<br>
流水线模型是并发编程模式的一种，并发编程可以使得计算机可以同时处理多个数据。而在 Golang 中，由于 Goroutine 的存在使得并发编程异常简单，Goroutine 之间需要数据传递，由于 channel 的存在也简化了许多。</p>
<h4 id="怎么组装一条流水线使用-channel-连接">怎么组装一条流水线？使用 Channel 连接</h4>
<p>在流水线中，每个处理单元称为一个 Stage，每个 Stage 都需要接受上游传来的数据，经过当前 Stage 处理后，再对下游传递。Channel 可以很好的承担传递数据的角色，也就是每个 Stage 都必须有两个 channel，一个负责接受数据的 channel 和一个负责对外发送数据的 channel。</p>
<p>Golang 提供的 channel 通信机制，可以帮助我们便捷的组装各个 stage，下面将通过实例说明。</p>
<h5 id="一个简单的例子求平方数">一个简单的例子：求平方数</h5>
<p>我们在这个例子中，将会输入 int 类型的参数，输出各个数的平方值，尽管这个逻辑可以用十分简单的代码实现，但我们还是将他分成3个 stage 来实现，以验证 golang pipeline 编程模型所带来的好处。</p>
<p>Stage1 ：</p>
<p>首先要产生数字，将这个 stage 取一个直观的名字 gen，输入参数是 int 列表，返回值是一个 channel。</p>
<pre><code class="language-go">func gen(nums …int) &lt;-chan int {
    out := make(chan int)
    go func() {
        for _, n := range nums {
            out &lt;- n
        }
        close(out)
    }()
    return out
}
</code></pre>
<p>Satge2 ：<br>
按照我们对 Pipeline 的定义，输入是一个 channel，输出还是一个 channel，处理逻辑即对收到的每个数进行平方</p>
<pre><code class="language-go">func sq(in &lt;-chan int) &lt;-chan int {
	out := make(chan int)
	go func() {
		for n := range in {
			out &lt;- n * n
		}
		close(out)
	}()
	return out
}
</code></pre>
<p>Stage3：<br>
最后一个 stage，也是 main 函数，他将会输出这些值。</p>
<pre><code class="language-go">func main() {
	in := gen(2, 3)
	c1 := sq(in)
	for n := range c1 {
    fmt.Println(n)
	}	
}
</code></pre>
<h4 id="扇入扇出-fan-out-fan-in多个-routine-同时处理">扇入扇出 Fan-Out、Fan-In：多个 Routine 同时处理</h4>
<p>我不认为扇出扇出是一个好的翻译，这两个单词单独去看并不能让我很直观的了解具体代表的逻辑。Fan-Out 和 Fan-In 是来自电子电路领域的一个概念，如果一个模块输出连接到了多个模块称为Fan-Out.<br>
移植这个概念到 软件的领域，自然指的是一个输出 Channel 连接多个 Staging。利用这个方式，我们可以将工作分发给更多的处理任务，提高并发度。当然，接收端也可以从多个 channel 接受数据。</p>
<pre><code class="language-go">func main() {
   in := gen(2, 3, 3,3)
   // Distribute the sq work across two goroutines that both read from in.
   c1 := sq(in)
   c2 := sq(in)

   out := merge( c1, c2)
   fmt.Println(&lt;-out)
}
</code></pre>
<p>分发给两个 sq，这是典型的 Fan-Out 操作，还引入了一个 merge 函数，这个函数是将两个 channel 输入，输出给一个 output channel，显然这是 Fan-In 操作。<br>
但是 merge 函数有一个重要的点需要注意，输出的 output channel 必须在所有数据发送完成后调用 close方法去关闭。如果过早的关闭，还有未发送的数据，如果发送到已经关闭的 channel上，会引起 panic。如果过晚的关闭或者不关闭，会造成资源泄露或者 main 函数一直在等待数据发送结束。<br>
这是一个很简单的同步模型，Golang 中提供了 WaitGroup 方式来实现这种逻辑。</p>
<pre><code class="language-go">func merge(cs ...&lt;-chan int) &lt;-chan int {
    var wg sync.WaitGroup
    out := make(chan int)

    // Start an output goroutine for each input channel in cs.  output
    // copies values from c to out until c is closed, then calls wg.Done.
    output := func(c &lt;-chan int) {
        for n := range c {
            out &lt;- n
        }
        wg.Done()
    }
    wg.Add(len(cs))
    for _, c := range cs {
        go output(c)
    }

    // Start a goroutine to close out once all the output goroutines are
    // done.  This must start after the wg.Add call.
    go func() {
        wg.Wait()
        close(out)
    }()
    return out
}
</code></pre>
<h5 id="提前结束">提前结束</h5>
<p>首先明确两个要点：<br>
output channel必须在数据全部发送完之前，一直保持 open<br>
只要 channel 还在 open 状态，就必须从里面接受数据<br>
接下来我们将处理两种特殊情况。<br>
之所以说是特殊情况，是因为在正常的逻辑中，我们会等到上游所有的数据发送完，下游所有的数据接受完以后关闭channel ，从而使得 goroutine 能够顺利被回收。但这种情况并不一定是我们每次都需要的，比如我们只需要结果的一个子集，又或者上游的 Stage 产生了异常，这种情况我们都没有必要接受完全部的数据再处理，这种情况我们称为提前结束。<br>
提前结束不是仅仅不再接受数据即可，但是这里还涉及到合理的关闭各个 channel，如果下游已经不想接受数据，上游的发送还没完成的话，上游将会永远阻塞，这会导致 goroutine 泄露，如果在常驻进程中，这可能是一笔极大的损耗。<br>
因此必须有一个机制来通知上游 stage：我们不再接受数据。首先想到的是增加一个 channel，用来在下游和上游中做消息通知。</p>
<pre><code class="language-go">func main() {
    in := gen(2, 3)

    // Distribute the sq work across two goroutines that both read from in.
    c1 := sq(in)
    c2 := sq(in)

    // Consume the first value from output.
    done := make(chan struct{}, 2)
    out := merge(done, c1, c2)
    fmt.Println(&lt;-out) // 4 or 9 

    // Tell the remaining senders we're leaving.
    done &lt;- struct{}{}
    done &lt;- struct{}{}
}
</code></pre>
<p>我们模拟了下游出现异常的情况，只在 out 中接受了一个数据就不再接受了，此时如果不关闭上游 channel，将会造成资源泄露。</p>
<p>我们在 main 函数中增加了一个 done channel，并且把该 channel 放入到 merge 方法中，用来接受信号。</p>
<p>这里连续对两个 done 发送消息的原因是因为，在下面的 merge 函数中，select 在 done 接受到消息后，会顺序执行下面的所有指令，即 wg.Done()，而在示例代码中有两个 channel 被放入到output 中处理，因此两个 channel 都要被关闭。</p>
<pre><code class="language-go">func merge(done &lt;-chan struct{}, cs ...&lt;-chan int) &lt;-chan int {
    var wg sync.WaitGroup
    out := make(chan int)

    // Start an output goroutine for each input channel in cs.  output
    // copies values from c to out until c is closed or it receives a value
    // from done, then output calls wg.Done.
    output := func(c &lt;-chan int) {
        for n := range c {
            select {
            case out &lt;- n:
            case &lt;-done:
            }
        }
        wg.Done()
    }
    // ... the rest is unchanged ...
</code></pre>
<p>这种方式有明显的缺点，你必须知道上游有多少个 channel Fan-In，才能在下游发送多少个 done 消息去关闭每一个 channel，下面我们改进一下。</p>
<h5 id="自动化一点利用-close-channel-通知">自动化一点：利用 Close Channel 通知</h5>
<pre><code class="language-go">func main() {
    // Set up a done channel that's shared by the whole pipeline,
    // and close that channel when this pipeline exits, as a signal
    // for all the goroutines we started to exit.
    done := make(chan struct{})
    defer close(done)

    in := gen(done, 2, 3)

    // Distribute the sq work across two goroutines that both read from in.
    c1 := sq(done, in)
    c2 := sq(done, in)

    // Consume the first value from output.
    out := merge(done, c1, c2)
    fmt.Println(&lt;-out) // 4 or 9

    // done will be closed by the deferred call.
}
</code></pre>
<p>在 merge 函数中增加对 done channel 的处理，因为我们不关心 done channel 的接收到的值是什么，当done channel 被 close 时，所有 stage 都可以在 done channel 中接受到 0 值。</p>
<p>当 done 中接受到值时，直接return，return 时defer 函数将会自动执行 wg.Done()。</p>
<pre><code class="language-go">func merge(done &lt;-chan struct{}, cs ...&lt;-chan int) &lt;-chan int {
    var wg sync.WaitGroup
    out := make(chan int)

    // Start an output goroutine for each input channel in cs.  output
    // copies values from c to out until c or done is closed, then calls
    // wg.Done.
    output := func(c &lt;-chan int) {
        defer wg.Done()
        for n := range c {
            select {
            case out &lt;- n:
            case &lt;-done:
                return
            }
        }
    }
    // ... the rest is unchanged ...
</code></pre>
<p>这里的 return 和 defer 是相互搭配的，也可以不使用 return，但必须手动增加 defer 对应的逻辑。</p>
<p>对应的，上游 stage 也应该执行相同的 close 操作。</p>
<pre><code class="language-go">func sq(done &lt;-chan struct{}, in &lt;-chan int) &lt;-chan int {
    out := make(chan int)
    go func() {
        defer close(out)
        for n := range in {
            select {
            case out &lt;- n * n:
            case &lt;-done:
                return
            }
        }
    }()
    return out
}
</code></pre>
<p>两个 Guideline：<br>
stages close their outbound channels when all the send operations are done.<br>
要么所有数据发送完，主动关闭。<br>
stages keep receiving values from inbound channels until those channels are closed or the senders are unblocked.<br>
要么持续从上游接受数据，直到上游关闭或者发动端全部发完。</p>
<p>引用连接：</p>
<p><a href="https://draveness.me/golang/docs/part2-foundation/ch05-keyword/golang-select/#%E9%9A%8F%E6%9C%BA%E6%89%A7%E8%A1%8C">Golang Select机制</a></p>
<p><a href="https://blog.csdn.net/chizong2116/article/details/100732815?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.control&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.control">Goroutine 泄露的几种情况</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bochs 安装]]></title>
        <id>https://chenat9.github.io/post/bochs-an-zhuang/</id>
        <link href="https://chenat9.github.io/post/bochs-an-zhuang/">
        </link>
        <updated>2021-02-01T13:13:49.000Z</updated>
        <content type="html"><![CDATA[<h3 id="环境">环境</h3>
<p>OSX Catalina</p>
<h3 id="安装指令">安装指令</h3>
<p>最开始尝试下载 bochs 的源码编译安装，但是提示找不到 &lt;linux/type.h&gt; 头文件。后来查找资料，发现使用 brew 即可安装 bochs</p>
<pre><code>brew install bochs
</code></pre>
<p>安装完成后，可以执行 bochs，打印出出错信息，说明 bochs 已经安装成功。<br>
<img src="https://chenat9.github.io/post-images/1612271089265.png" alt="" loading="lazy"></p>
<h3 id="配置bochs">配置bochs</h3>
<p>配置文件：bochsrc.disk</p>
<pre><code>megs: 32

romimage: file=$PATH/bochs-2.6.10/bios/BIOS-bochs-latest
vgaromimage: file=$PATH/bochs-2.6.10/bios/VGABIOS-lgpl-latest

boot: disk

log: bochs.out

mouse: enabled=0

keyboard: keymap=$PATH/bochs-2.6.10/gui/keymaps/sdl2-pc-us.map

ata0: enabled=1, ioaddr1=0x1f0, ioaddr2=0x3f0, irq=14

#gdbstub: enabled=1, port=1234, text_base=0, data_base=0, bss_base=0

</code></pre>
<p>上述参数具体解释在后面已经说明，需要注意的是一些配置文件需要指定路径，使用 brew 方式安装的 bochs 没有这些文件（或者我没有找到...)，因此可以下载 bochs 的源码，按上述中的路径找到后配置在文件中。</p>
<h3 id="运行-bochs">运行 bochs</h3>
<p>当配置完成后，可以运行 bochs 命令来启动</p>
<pre><code>bochs -f bochsrc.disk
</code></pre>
<p>运行后，按 C 键，系统将会跳出 bochs 的 UI 窗口，如下图所示，可以看到 No bootable device.的错误，这是由于没有指定启动盘。<br>
<img src="https://chenat9.github.io/post-images/1612271130070.png" alt="" loading="lazy"></p>
<h3 id="指定启动盘">指定启动盘</h3>
<p>要指定启动盘，首先要有一个启动盘。安装 bochs 的时候，会自带一个制作 image 的工具:bximage<br>
<img src="https://chenat9.github.io/post-images/1612271165343.png" alt="" loading="lazy"><br>
运行bximage 命令后，选择创建默认 IMG，输入一些配置以及输出路径即可（可以都按默认配置），生成 IMG 文件成功后，bximage 会贴心的告诉你</p>
<pre><code>The following line should appear in your bochsrc:
  ata0-master: type=disk, path=&quot;c.img&quot;, mode=flat
</code></pre>
<p>将这一行添加到配置文件中，再次启动 bochs，按 C 运行。可以看到依然显示 No bootable device，但错误原因已经变了。<br>
<img src="https://chenat9.github.io/post-images/1612271118120.png" alt="" loading="lazy"><br>
这次是因为img 中没有可以启动的代码，接下来就去编写 MBR 引导程序吧。<br>
<img src="https://chenat9.github.io/post-images/1612271191661.png" alt="" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Async/Await]]></title>
        <id>https://chenat9.github.io/post/asyncawait/</id>
        <link href="https://chenat9.github.io/post/asyncawait/">
        </link>
        <updated>2020-11-23T16:03:41.000Z</updated>
        <content type="html"><![CDATA[<p>在这篇文章，我们将探索Rust 中的多任务机制和异步/等待特性。我们详细的剖析异步/等待机制如何工作，包括 Future 的设计，状态机转换和 pinning。我们增加了基础的支持在内核中。通过创建异步键盘任务和基本的执行器。</p>
<h3 id="多任务">多任务</h3>
<p><code>多任务</code> 是大多数操作系统的基本特性之一，这是一种允许多任务并行执行的能力。 举例来说，你可能打开了一个程序来看这片文章，比如文字编辑器或者 Terminal 窗口，甚至可能有打开了一个单独的浏览器窗口。各种各样任务在后台运行，这样你才能够管理桌面窗口，检查更新或者索引文件。<br>
任务看起来是并行的，但实际上CPU在同一时刻只允许一个任务在执行。为了让用户产生程序在同时运行的错觉，操作系统必须在不同的后台程序之间快速的切换。<br>
但是，单核 CPU 同时只能运行一个任务，但多核 CPU 可以真正同时运行多个任务。<br>
多任务处理系统有两种常见形式：协作式多任务处理系统需要任务主动放弃 CPU 的控制权，以便其他任务可以进行。抢占式多任务使用操作系统提供的能力去切换，操作系统可以任意的强制停止他们。接下来，我们将详细介绍两者的不通。</p>
<h4 id="抢占式多任务">抢占式多任务</h4>
<p>抢占式多任务的核心观念是由操作系统来控制何时切换任务。正因如此，它充分利用了在每个中断发生时，操作系统都会重新获得 CPU 控制权这个特点。这也使得无论何时，当系统中有新的输入可用时得以切换任务。比如，鼠标移动或者网络包到达。<br>
不仅如此，操作系统也可以在配置硬件系统，令其在达到某个时间后主动发送中断，</p>
<p>在第一行，CPU 正在执行 A程序中的 A1任务。其他任务均被暂停。在第二行中，硬件中断到达 CPU。CPU 立刻停止A1任务，然后跳到在 IDT 中定义的了中断处理器。通过指定的中断处理器，操作系统现在重新获得 CPU 控制权，然后然后操作系统允许CPU切换到 B1 任务继续运行。</p>
<h5 id="状态保存">状态保存</h5>
<p>程序可能被任意终止，无论程序是否进行计算。因此稍后继续恢复运行，操作系统必须保存程序运行的全部状态，包括程序调用栈，和 CPU 寄存器的值。这个过程就称为上下文切换。<br>
由于调用栈可能特别庞大， 操作系统每个任务单独设置了自己的调用栈，避免每次任务切换时，需要保存所有任务的调用栈。通过使用单独的调用栈，在任务切换时，只有寄存器内容需要被保存。这个方式最小化了性能损耗，这是上下文切换能够达到100 times/s 的关键。</p>
<h5 id="讨论">讨论</h5>
<p>抢占式多任务的最主要的优势在于操作系统可以完全控制任务的运行时间。内核可以保证每一个任务可以获得公平的 CPU 时间，不需要信任其他任务。这对于运行一些多用户或者第三方系统式很有用，因为第三方应用程序可能会占据 CPU 时间不释放。</p>
<p>劣势在于抢占式任务需要每个都有自己的栈。与共享栈对比，导致了更高的内存使用率，而且经常达到任务数的上限。另外一个劣势是，操作系统总是需要保存完整的 CPU 寄存器状态，即便在任务发生切换时，程序只使用了一部分寄存器。</p>
<h4 id="协作式多任务">协作式多任务</h4>
<p>不同于抢占式任务可以在任意时间点强制停止任务，协作式多任务系统允许每个任务主动放弃 CPU 的控制权。任务可以自由选择可以暂停的时间点，比如当程序需要等待 IO 操作时。</p>
<p>协作式多任务的实现通常是在语言层面，比如 Rust 的async/await。这个功能使得无论是程序员或者编译器在程序中插入 yield，都可以放弃 CPU控制权，允许其他程序运行。比如，在每一轮复杂的循环过后进行 yield。</p>
<p>将多任务系统和异步操作绑定在一起，是非常自然的。不同于等待一个操作完成和阻止其他任务同时运行，异步操作会返回一个还没准备好状态。比如，阻塞任务执行一个 yield 操作让其他任务运行。</p>
<h5 id="状态保存-2">状态保存</h5>
<p>由于任务可以自定义他们的暂停点，也就不再需要操作系统去保存他们的运行状态。取而代之的是他们只需要精确的存储他们真正需要的状态，这样就会有更好的性能。比如，一个任务再复杂的计算后，也许只需要保存结果。</p>
<p>对于实现而言，协作式任务有语言层面的支持，协作式任务能够备份他们需要的部分调用栈。通过只保存相关部分，所有的任务可以共享一个调用栈，这样可以消耗更小的内存。</p>
<h5 id="讨论-2">讨论</h5>
<p>协作式任务的缺点是，一个程序可以不释放CPU，一个恶意程序会阻止其他程序运行，甚至拖慢整个系统。协作式任务必须被所有的任务互相信任。比如，操作系统内核就不能基于用户级程序实现。</p>
<p>然而，强力的性能和内存友好是协作式程序一大有点，尤其是和异步操作绑定时。因此一个操作系统内核搭配支持异步的硬件，协作式任务是一个好的方式去实现并发功能。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[HTTP 流式传输]]></title>
        <id>https://chenat9.github.io/post/http-liu-shi-chuan-shu/</id>
        <link href="https://chenat9.github.io/post/http-liu-shi-chuan-shu/">
        </link>
        <updated>2020-11-23T03:04:09.000Z</updated>
        <content type="html"><![CDATA[<h4 id="问题场景">问题场景</h4>
<p>日常工作跟分布式存储相关，遇到这样一个业务场景：</p>
<blockquote>
<p>将大文件通过 HTTP 协议传输到服务端。无法一次加载到内存中，组装到 Request 的 body 中。</p>
</blockquote>
<p>针对这样的问题，应该怎么解决呢？最简单的思路就是分而治之，将大文件分割成小文件上传，但是同一个大文件显然不能通过多次 HTTP 请求发送(这样会被认为是多个文件)。<br>
HTTP 流式传输为我们提供了相应的解决方案。</p>
<p>先来看几个知识点：</p>
<h4 id="keepalive模式">KeepAlive模式</h4>
<p>大学的知识学完，对 HTTP 的认知只停留在了少数几个关键词：HTTP 是无状态无连接的。</p>
<p>无连接的意思是指HTTP协议采用“请求-应答”模式，当使用普通模式，即非KeepAlive模式时，每个请求/应答客户和服务器都要新建一个连接，完成之后立即断开连接；</p>
<p>后来 Web 的世界越来越精彩，一个网页中可能嵌套了多种资源，比如图片、视频，为了解决频繁建立 TCP 连接带来的性能损耗，出现了 Keep-Alive 模式。当使用Keep-Alive模式（又称持久连接、连接重用）时，Keep-Alive功能使客户端到服务器端的连接持续有效，当出现对服务器的后继请求时，Keep-Alive功能避免了建立或者重新建立连接。</p>
<p>HTTP 本身是无状态的，那么Keep-Alive 的功能必然是依赖 TCP 来实现的。TCP 的实现中包含一个 Keep-Alive 定时器，当一条数据流中没有数据通过时，服务端每隔一段时间会向客户端发送一个不带数据的 ACK 请求，如果收到 Client回复，表明连接依然存在。如果没有收到回复，Server会多次 ACK，达到一定次数以后还没有收到回复，默认此连接关闭。</p>
<figure data-type="image" tabindex="1"><img src="https://chenat9.github.io/post-images/1606100739103.png" alt="" loading="lazy"></figure>
<p>另外需要说明的是，Keep-Alive 模式在HTTP 1.0中默认是关闭的，需要在HTTP头加入&quot;Connection: Keep-Alive&quot;，才能启用Keep-Alive；HTTP 1.1中默认启用Keep-Alive，如果加入&quot;Connection: close &quot;，才关闭。但是若想完成一次 Keep-Alive 的连接，仍旧需要 Client 和 Server 端共同支持，如果某一端处理完请求直接关闭了 Socket，神仙也保证不了连接。</p>
<p>大文件的上传既然不可能一次性放到 RequestBody 中传输，那么就避免不了多次传输。Keep-Alive 模式解决了不必重复频繁建立连接的问题，第二个问题随之而来，怎么判断数据流的结束？</p>
<p>在请求-响应模式下，每一次 HTTP 请求发送完成，Client 都会主动关闭连接，Server 端在读取完所有的 Body 数据后，就认为此次请求已经完毕，开始在服务端进行处理。但是在 Keep-Alive模式下，这个问题显然就没有这么简单了。举个例子，比如一条 Keep-Alive 的HTTP 连接 ，通过底层的 TCP 通道连续发送了两张图片，对于服务器来说，如何判断这是两张图片？而不是把他们当做同一个文件的数据进行处理呢？再比如，普通模式下，客户端请求服务器的数据，服务端发送完响应就会关闭连接，客户端读取时会读到 EOF(-1)，这个时候客户端就会关闭自身的连接。但在 Keep-Alive 模式下，服务器不会主动关闭连接，Client 自然也就读不到 EOF，客户端该如何判断数据流的结束呢。</p>
<h4 id="判断数据流结束的方法">判断数据流结束的方法</h4>
<p>HTTP 为我们提供了两种方式</p>
<ol>
<li>
<p>Content-Length</p>
<p>这是一个很直观的方式，在要传输的数据前增加一个信息，来告知对端将要传输多少数据，这样在另一侧读取到这个长度的数据后，可以认为接受已经完成。</p>
<p>如果无法提前预知Content-Length 呢，比如数据源还在不断的生成当中，不知道什么时候会结束。接下来还有第二种办法。</p>
</li>
<li>
<p>使用消息Header 字段，Transfer-Encoding:chunk</p>
<p>如果要一边产生数据，一边发给客户端，服务器就需要使用&quot;Transfer-Encoding: chunked&quot;这样的方式来代替Content-Length。</p>
<p>chunk编码将数据分成一块一块的发出。Chunked编码将使用若干个Chunk串连而成，由一个标明<strong>长度为0</strong>的chunk标示结束。每个Chunk分为头部和正文两部分，头部内容指定正文的字符总数（<strong>十六进制的数字</strong>）和数量单位（一般不写），正文部分就是指定长度的实际内容，两部分之间用**回车换行(CRLF)**隔开。在最后一个长度为0的Chunk中的内容是称为footer的内容，是一些附加的Header信息（通常可以直接忽略）。</p>
</li>
</ol>
<h4 id="抓包验证">抓包验证</h4>
<p>百闻不如一见，使用 WireShark 先抓为敬。<br>
chunk 编码方式抓包：<br>
Server 编码：</p>
<pre><code>func indexHandler(w http.ResponseWriter, r *http.Request) {
   //fmt.Print(r.Body.Read())
 fmt.Fprint(w, &quot;hello world&quot;)
}
func main() {
   http.HandleFunc(&quot;/report&quot;, indexHandler)
   http.ListenAndServe(&quot;:8000&quot;, nil)
}
</code></pre>
<p>Client 代码</p>
<pre><code>func main() {
   pr, rw := io.Pipe()
   go func(){
      for i := 0; i &lt; 100; i++ {
         rw.Write([]byte(fmt.Sprintf(&quot;line:%drn&quot;, i)))
      }
      rw.Close()
   }()
   http.Post(&quot;localhost:8000/&quot;,&quot;text/pain&quot;, pr)
}
</code></pre>
<figure data-type="image" tabindex="2"><img src="https://chenat9.github.io/post-images/1606100756625.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="3"><img src="https://chenat9.github.io/post-images/1606100761885.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="4"><img src="https://chenat9.github.io/post-images/1606100766355.png" alt="" loading="lazy"></figure>
<p>从结果可以看出，HTTP 协议底层通过同一个 TCP 连接在发送数据。<br>
每一个TCP packet 的内容为我们写入的数据。</p>
<p>方式二：Content-Length<br>
Client编码</p>
<pre><code>func main() {
   count := 10
 line := []byte(&quot;linern&quot;)
   pr, rw := io.Pipe()
   go func() {
      for i := 0; i &lt; count; i++ {
         rw.Write(line)
         time.Sleep(500 * time.Millisecond)
      }
      rw.Close()
   }()
   // 构造request对象
 request, err := http.NewRequest(&quot;POST&quot;, &quot;http://localhost:8000/report&quot;, pr)
   if err != nil {
      log.Fatal(err)
   }
   // 提前计算出ContentLength
 request.ContentLength = int64(len(line) * count)
   // 发起请求
 http.DefaultClient.Do(request)
}
</code></pre>
<figure data-type="image" tabindex="5"><img src="https://chenat9.github.io/post-images/1606100773785.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="6"><img src="https://chenat9.github.io/post-images/1606100778336.png" alt="" loading="lazy"></figure>
<p>当然底层依然是通过多次 tcp 包传输的。</p>
<h4 id="总结">总结</h4>
<p>为了解决不能将大数据一次性拼接到 Request 的 Body 中这个问题，采取了 HTTP 流式传输的方式，即边读进内存，边通过 HTTP 传输。</p>
<p>Keep-Alive 模式，避免了传输多个报文时，TCP 连接重复建立，为流式传输大量数据打好了基础。</p>
<p>Content-Length 和 Transfer-Encoding，两种方式为消息内容的长度判断提供了解决方案。</p>
<p>设置Content-Length的方式，由 Client 端持续将数据通过 HTTP 传输到 Server。chunk方式，Client 端将数据分片发包到 Server。</p>
<p>以上的方式都是通过建立一次 HTTP 传输完成的。</p>
]]></content>
    </entry>
</feed>